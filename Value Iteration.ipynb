{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "J-SOCfgJtHJw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.style.use('seaborn-notebook')\n",
        "# plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "outputs": [],
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "nvTUR66UtPUk"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)] # Initialize state values.\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1 # Ensure valid noise value.\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "\n",
        "    def get_index_state(self, state):\n",
        "        \"\"\"\n",
        "        Get the index of the state in the grid\n",
        "        \"\"\"\n",
        "        return state[0] * self._width + state[1]\n",
        "\n",
        "    def _state_reward_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "\n",
        "        random_noise = np.random.rand()\n",
        "        actions_list = list(action_to_str.keys())\n",
        "        action_base = action\n",
        "\n",
        "        # adding noise to the model, choose random actions with probability < noise\n",
        "        if random_noise < self._noise:\n",
        "            action = np.random.choice(actions_list)\n",
        "\n",
        "        is_action_same = action == action_base\n",
        "        # get the next state based on the actions table\n",
        "        next_state_r, next_state_c = (state[0] + action_to_offset[action][0]), (state[1] + action_to_offset[action][1])\n",
        "\n",
        "        # if the actions leads to out of bounds states then we keep the state we have\n",
        "        if next_state_r < 0 or next_state_r >= self._height:\n",
        "            next_state_r = state[0]\n",
        "        if next_state_c < 0 or next_state_c >= self._width:\n",
        "            next_state_c = state[1]\n",
        "\n",
        "        next_state = next_state_r, next_state_c\n",
        "        # get the index of the next state in the grid to be consistent with the test cases\n",
        "        next_state_index = self.get_index_state(next_state)\n",
        "\n",
        "        # if the next state is a blocke cell then next state will still be the current state\n",
        "        if next_state_index in self._blocked_cells:\n",
        "            next_state = state\n",
        "            next_state_index = self.get_index_state(next_state)\n",
        "\n",
        "        return next_state_index, self.get_reward(next_state_index)\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        return (state == self._goal_cell) or (state in self._danger_cells)\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        elif state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def execute_actions(self, state, row, col, discount_factor, A, b):\n",
        "        \"\"\"\n",
        "        Execute the action in the current state\n",
        "        \"\"\"\n",
        "        actions_list = list(action_to_str.keys())\n",
        "        number_actions = len(actions_list)\n",
        "\n",
        "        for action in actions_list:\n",
        "            # for each action we get the next state and the reward and calculate A and b for the linear solver\n",
        "            next_state_index, reward = self._state_reward_from_action((row, col), action)\n",
        "\n",
        "            # Based on the equations in the lecture, we substitute to solve for the Value FFunction\n",
        "            # Here 1/number_actions will be 1/4 but I will use the variable for consistency in case\n",
        "            # I wanted to add diagonal actions in the future\n",
        "\n",
        "            A[state, next_state_index] -= ((1/number_actions) * discount_factor)\n",
        "            b[state] += reward/number_actions\n",
        "\n",
        "        return A, b\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        n_states = self._height * self._width\n",
        "        A = np.zeros((n_states, n_states))\n",
        "        b = np.zeros(n_states)\n",
        "\n",
        "        # looping over the state to calculate the\n",
        "        for row in range(self._height):\n",
        "            for col in range(self._width):\n",
        "                state_index = row * self._width + col\n",
        "\n",
        "                A[state_index, state_index] = 1.0\n",
        "                if self.is_terminal(state_index) or (state_index in self._blocked_cells):\n",
        "                    b[state_index] = self.get_reward(state_index)\n",
        "\n",
        "                else:\n",
        "                    A, b = self.execute_actions(state_index, row, col, discount_factor, A, b)\n",
        "\n",
        "        self._grid_values = np.linalg.solve(A, b)\n",
        "        return self._grid_values\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6}\".format(\"-5.00\")\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        print(out_str)\n",
        "        return out_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "Dfgo0v5sNO78"
      },
      "outputs": [],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "\n",
        "# # Solve the linear system\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3llS2gdf4TUQ",
        "outputId": "50f25c6b-1a94-4bf2-baa0-bc380ce46fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-9.97136722e+00, -9.98568361e+00, -5.00000000e+00,\n",
              "        -3.33333333e+00,  6.35389177e-16],\n",
              "       [-9.95705082e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         5.12410627e-16,  3.33333333e+00],\n",
              "       [-9.94273443e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  5.00000000e+00],\n",
              "       [-9.92841804e+00, -9.87831067e+00, -9.70651396e+00,\n",
              "        -5.00000000e+00, -1.60343593e+00],\n",
              "       [-9.96420902e+00, -5.00000000e+00, -9.24123121e+00,\n",
              "        -8.01717967e+00, -4.81030780e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "values_grid.reshape((5, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "HBCUr1sY4TUQ",
        "outputId": "298b81c9-45f4-4c8c-ba49-c90f5b70f1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -9.97  -9.99  -5.00  -3.33   0.00 \n",
            " -9.96   ----   ----   0.00   3.33 \n",
            " -9.94   ----   ----   0.00   GOAL \n",
            " -9.93  -9.88  -9.71  -5.00  -1.60 \n",
            " -9.96  -5.00  -9.24  -8.02  -4.81 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' -9.97  -9.99  -5.00  -3.33   0.00 \\n -9.96   ----   ----   0.00   3.33 \\n -9.94   ----   ----   0.00   GOAL \\n -9.93  -9.88  -9.71  -5.00  -1.60 \\n -9.96  -5.00  -9.24  -8.02  -4.81 \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "simple_gw.__str__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "HJI-fizhOIM-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration(grid, discount, tolerance=0.1):\n",
        "    \"\"\"\n",
        "    Value iteration algorithm for gridworld as follows:\n",
        "    1. Start with arbitrary (zero) initialized value function.\n",
        "    2. Loop over each state and do 1 step lookahead\n",
        "    \"\"\"\n",
        "\n",
        "    n_states = grid._height * grid._width\n",
        "    values = np.zeros(n_states)\n",
        "    actions_list = list(action_to_str.keys())\n",
        "    count = 0\n",
        "\n",
        "    # now we loop indefinetly till delta is less than the tolerance that we pre-specify\n",
        "    # at each iteration (state) we calculate the new value based on the old ones and the immediate reward\n",
        "    while True:\n",
        "        # we now make a copy of the value table to calculate the new values given the old values\n",
        "        # This is done so that we don't alter the old values during calculation\n",
        "        value_table_tmp = np.copy(values)\n",
        "        delta = 0\n",
        "\n",
        "        # Looping over each state where a state is a tuple of row and column in the grid\n",
        "        for row in range(grid._height):\n",
        "            for col in range(grid._width):\n",
        "                state = row, col\n",
        "                # getting the index of the state in the grid\n",
        "                state_index = grid.get_index_state(state)\n",
        "\n",
        "                # if the state is a termina or blocked then we will not calculate the value from them\n",
        "                # because if it is a terminal state then the game ends and if it is blocked then\n",
        "                # you are not allowed to visit it so I wouldn't calculate the value from it\n",
        "                if grid.is_terminal(state_index) or (state_index in grid._blocked_cells):\n",
        "                    continue\n",
        "                else:\n",
        "                    # According to the value iteration algorithm, we need to make a 1 step lookahead\n",
        "                    # so at each state we check all available actions and choose the one with the largest value\n",
        "                    curr_max_value = -np.inf\n",
        "                    for action in actions_list:\n",
        "                        next_state_index, reward = grid._state_reward_from_action(state, action)\n",
        "                        next_state_value = reward + discount * values[next_state_index]\n",
        "                        curr_max_value = max(curr_max_value, next_state_value)\n",
        "                    value_table_tmp[state_index] = curr_max_value\n",
        "                    delta = max(delta, np.abs(value_table_tmp[state_index] - values[state_index]))\n",
        "\n",
        "        values = value_table_tmp\n",
        "\n",
        "        count += 1\n",
        "        if delta < tolerance:\n",
        "            break\n",
        "\n",
        "    # Now I add the values of the fire cells as they were not included above since they are terminal states\n",
        "    # They will just be used in calculating the optimal policy.\n",
        "    values[grid._goal_cell] = grid._goal_value\n",
        "    for danger in grid._danger_cells:\n",
        "        values[danger] = grid._danger_value\n",
        "\n",
        "    print(f\"Number of iterations till reaching tolerance = {count}\")\n",
        "    return values\n",
        "\n",
        "\n",
        "def update_policy(actions_list, grid, state, value_func, discount):\n",
        "    max_value = float('-inf')\n",
        "    best_action = None\n",
        "\n",
        "    # for the current state, loop over all the actions and choose the one\n",
        "    # that maximizes the value given reward, gamma and the calculate value function\n",
        "    for action in actions_list:\n",
        "      next_state_index, reward = grid._state_reward_from_action(state, action)\n",
        "      next_value = reward + discount * value_func[next_state_index]\n",
        "\n",
        "      # find the action with maximum value\n",
        "      if next_value > max_value:\n",
        "        max_value = next_value\n",
        "        best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "def get_optimal_policy(grid, value_func, discount):\n",
        "    policy = np.zeros((grid._height, grid._width))\n",
        "    actions_list = list(action_to_str.keys())\n",
        "\n",
        "    # for each state, find the action with maximum value\n",
        "    for row in range(grid._height):\n",
        "        for col in range(grid._width):\n",
        "            state = (row, col)\n",
        "            state_index = grid.get_index_state(state)\n",
        "            if grid.is_terminal(state_index) or (state_index in grid._blocked_cells):\n",
        "                policy[row, col] = np.nan\n",
        "            else:\n",
        "                policy[row, col] = update_policy(actions_list, grid, state, value_func, discount)\n",
        "    return policy\n",
        "\n",
        "def print_policy(policy, action_to_str):\n",
        "    # printing the policy in 2d grid\n",
        "    action_to_str = {k: v for k, v in action_to_str.items()}\n",
        "    convert = lambda x: action_to_str[x][0].upper()\n",
        "    action_to_str[''] = \"-\"\n",
        "    str_policy = []\n",
        "\n",
        "    for i in policy:\n",
        "        cleanedList = [x if not np.isnan(x) else '' for x in i]\n",
        "        str_policy.append(list(map(convert, cleanedList)))\n",
        "\n",
        "    print(np.array(str_policy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "OwueBZR8tPXE"
      },
      "outputs": [],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "ljGtHN1t4TUR",
        "outputId": "0f8785e6-31d8-4683-cc2a-34bb8a4b1dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0.00   0.00  -5.00   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   0.00 \n",
            "  0.00   ----   ----   0.00   GOAL \n",
            "  0.00   0.00   0.00  -5.00   0.00 \n",
            "  0.00  -5.00   0.00   0.00   0.00 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  0.00   0.00  -5.00   0.00   0.00 \\n  0.00   ----   ----   0.00   0.00 \\n  0.00   ----   ----   0.00   GOAL \\n  0.00   0.00   0.00  -5.00   0.00 \\n  0.00  -5.00   0.00   0.00   0.00 \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "simple_gw.__str__()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_policy(discount, tolerance, noise = 0):\n",
        "  simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=noise)\n",
        "\n",
        "  value_arr = value_iteration(simple_gw, discount, 0.1);\n",
        "  print(value_arr.round(2).reshape((5, 5)))\n",
        "  policy = get_optimal_policy(simple_gw, value_arr, discount);\n",
        "  print_policy(policy, action_to_str)"
      ],
      "metadata": {
        "id": "8COvNycFCbEw"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "calculate_policy(0.95, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9z2HOwrFxEX",
        "outputId": "9f7d9a35-2952-4f74-d0f1-0bda1a8f34ea"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations till reaching tolerance = 12\n",
            "[[ 3.15  2.99 -5.    4.51  4.75]\n",
            " [ 3.32  0.    0.    4.75  5.  ]\n",
            " [ 3.49  0.    0.    5.    5.  ]\n",
            " [ 3.68  3.87  4.07 -5.    5.  ]\n",
            " [ 3.49 -5.    4.29  4.51  4.75]]\n",
            "[['D' 'L' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' '-']\n",
            " ['R' 'R' 'D' '-' 'U']\n",
            " ['U' '-' 'R' 'R' 'U']]\n",
            "CPU times: user 7.73 ms, sys: 921 µs, total: 8.65 ms\n",
            "Wall time: 15.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "calculate_policy(0.75, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HNdcB7YF8sV",
        "outputId": "7b411aae-590b-491f-ba74-9928936cda43"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations till reaching tolerance = 12\n",
            "[[ 0.38  0.28 -5.    2.81  3.75]\n",
            " [ 0.5   0.    0.    3.75  5.  ]\n",
            " [ 0.67  0.    0.    5.    5.  ]\n",
            " [ 0.89  1.19  1.58 -5.    5.  ]\n",
            " [ 0.67 -5.    2.11  2.81  3.75]]\n",
            "[['D' 'L' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' '-']\n",
            " ['R' 'R' 'D' '-' 'U']\n",
            " ['U' '-' 'R' 'R' 'U']]\n",
            "CPU times: user 8.09 ms, sys: 0 ns, total: 8.09 ms\n",
            "Wall time: 8.46 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_policy(0.75, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjS3W6qxGTfb",
        "outputId": "abf82dfa-5e4d-48af-9c75-29aee9bbcfd4"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations till reaching tolerance = 12\n",
            "[[ 0.38  0.28 -5.    2.81  3.75]\n",
            " [ 0.5   0.    0.    3.75  5.  ]\n",
            " [ 0.67  0.    0.    5.    5.  ]\n",
            " [ 0.89  1.19  1.58 -5.    5.  ]\n",
            " [ 0.67 -5.    2.11  2.81  3.75]]\n",
            "[['D' 'L' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' '-']\n",
            " ['R' 'R' 'D' '-' 'U']\n",
            " ['U' '-' 'R' 'R' 'U']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "calculate_policy(0.95, 0.1, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6nX1BunGFPU",
        "outputId": "cb457178-384e-4302-9387-541a9d73d8e2"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations till reaching tolerance = 626\n",
            "[[ 3.15  2.99 -5.    4.51  4.75]\n",
            " [ 3.32  0.    0.    4.75  5.  ]\n",
            " [ 3.49  0.    0.    5.    5.  ]\n",
            " [ 3.68  3.87  4.07 -5.    5.  ]\n",
            " [ 3.49 -5.    4.29  4.51  4.75]]\n",
            "[['D' 'L' '-' 'R' 'D']\n",
            " ['D' '-' '-' 'R' 'R']\n",
            " ['D' '-' '-' 'R' '-']\n",
            " ['R' 'R' 'D' '-' 'U']\n",
            " ['U' '-' 'R' 'R' 'U']]\n",
            "CPU times: user 423 ms, sys: 7.4 ms, total: 430 ms\n",
            "Wall time: 462 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUo6u5kutPbm"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "calculate_policy(0.95, 0.1, 0.2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}